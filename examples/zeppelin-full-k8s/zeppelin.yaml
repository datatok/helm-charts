image:
  repository: ghcr.io/datatok/zeppelin-server
  tag: 0.10.1-edge

resources:
  requests:
    cpu: 128m
    memory: 512Mi
  limits:
    cpu: 2
    memory: 800Mi

notebookStorage:
  # -- Kind of storage for notebook
  type: fs

volumeNotebooks:
  type: pvc
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 32Mi
    storageClassName: standard

volumeConf:
  type: pvc
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 32Mi
    storageClassName: standard

logging:
  level: INFO

zeppelin:
  interpreter:
    image:
      repository: zep-int-r # ghcr.io/datatok/zeppelin-interpreter-r
      tag: latest # 0.10.1-edge

spark:
  config:
    spark.app.name: zep

    # Deployment
    spark.master: k8s://https://kubernetes.default.svc

    # spark driver = zep interpreter
    spark.submit.deployMode: client
    
    # K8S
    spark.kubernetes.namespace: default

    # service account must be able to CRUD pods, services, configMap
    spark.kubernetes.authenticate.driver.serviceAccountName: zep-zeppelin

    # File must exist in zep-int / spark-driver container
    spark.kubernetes.executor.podTemplateFile: /opt/spark/k8s/executor-pod-template/pod.yaml
    spark.kubernetes.executor.limit.cores: 1

    spark.kubernetes.container.image: ghcr.io/datatok/spark:v3.2.1-1
    spark.kubernetes.file.upload.path: /tmp/zep-spark-k8s/

    spark.kubernetes.authenticate.submission.clientKeyFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    spark.kubernetes.driver.deleteOnTermination: false
    spark.kubernetes.executor.podNamePrefix: zep
    
    # Driver
    spark.driver.cores: 3
    spark.driver.memory: 800M

    #spark.driver.extraJavaOptions: -Dhttp.proxyHost=10.10.10.10 -Dhttp.proxyPort=2001
    #  -Dhttps.proxyHost=10.10.10.10 -Dhttps.proxyPort=2001 -Dhttp.noProxy=10.10.*
    
    # Executor
    spark.executor.cores: 1
    spark.executor.memory: 500M

    # FS
    spark.hadoop.dfs.nameservices: 10.10.10.10:8020
    spark.hadoop.fs.defaultFS: hdfs://10.10.10.10:8020
    spark.hadoop.hive.metastore.uris: thrift://10.10.10.10:9083
    spark.hive.metastore.uris: thrift://10.10.10.10:9083
    #spark.jars.packages: org.elasticsearch:elasticsearch-spark-30_2.12:7.17.1
    
    # Config
    spark.memory.fraction: 0.1
    spark.sql.shuffle.partitions: 10
    spark.eventLog.enabled: 'false'

  int:
    zeppelin.spark.concurrentSQL: 'true'
    zeppelin.spark.enableSupportedVersionCheck: 'false'
    zeppelin.interpreter.connect.timeout: 600000

    zeppelin.k8s.interpreter.serviceAccount: zep-zeppelin
    zeppelin.k8s.interpreter.cores: 2
    zeppelin.k8s.interpreter.memory: 4Gi

    zeppelin.k8s.interpreter.cores: 2
    zeppelin.k8s.interpreter.memory: 400Mi