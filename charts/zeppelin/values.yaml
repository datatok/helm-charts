# -- Pod replica count
replicaCount: 1

image:
  repository: apache/zeppelin
  pullPolicy: IfNotPresent

# -- To use private images
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false
  annotations: {}
  name: "default"
  rbac:
    create: false

podAnnotations: {}

podLabels: {}

# -- see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
podSecurityContext: {}
#  runAsNonRoot: true
#  runAsUser: 1000
#  runAsGroup: 1000
#  fsGroup: 1000

# -- see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext: {}
#  capabilities:
#    drop:
#     - ALL
#  allowPrivilegeEscalation: false
#  readOnlyRootFilesystem: true

networkPolicy:
  # -- enable network policy
  enabled: false
  # -- add extra NP egress rules
  extraEgressRules: []
  # -- add extra NP ingress rules
  extraIngressRules: []

# -- extraVolumes Array to add extra volumes
extraVolumes: []
#- name: toto
#  emptyDir: {}

# -- extraVolumeMounts Array to add extra mount
extraVolumeMounts: []
#- name: toto
#  mountPath: /app/toto

# -- extra env variables for zeppelin container
extraEnvVars: {}

service:
  type: ClusterIP
  # -- HTTP server port
  port: 8080
  # -- RPC port, to register interpreter
  rpcPort: 38853

ingress:
  # -- Enable Zeppelin server ingress
  enabled: false
  # -- Annotations to use cert-manager and sticky sessions
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    # nginx.ingress.kubernetes.io/affinity: "cookie"
    # nginx.ingress.kubernetes.io/session-cookie-name: "zep"
    # nginx.ingress.kubernetes.io/session-cookie-expires: "172800"
    # nginx.ingress.kubernetes.io/session-cookie-max-age: "172800"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# -- Resources, dont be greedy for memory, this is java :-) 
resources: {}
#  limits:
#    cpu: 1000m
#    memory: 1024Mi
#  requests:
#    cpu: 100m
#    memory: 256Mi

nodeSelector: {}

tolerations: []

affinity: {}

## -- Define volume to store notebooks (emptyDir or pvc)
volumeNotebooks:
  type: emptyDir

#volumeNotebooks:
#  type: pvc
#  spec:
#    accessModes:
#    - ReadWriteOnce
#    resources:
#      requests:
#        storage: 8Gi
#    storageClassName: rbd

## -- Define volume to store tmp (emptyDir or pvc)
volumeTmp:
  type: emptyDir

zeppelin:
  config:
    # -- IP to listen to (usually `0.0.0.0`)
    zeppelin.server.addr: 0.0.0.0
    # -- notebooks storage dir
    zeppelin.notebook.dir: notebook
    # -- Context Path of the Web Application (usually `/`)
    zeppelin.server.context.path: /
    # -- id of notebook to be displayed in homescreen. ex) 2A94M5J1Z Empty value displays default home screen
    zeppelin.notebook.homescreen: ""
    # -- hide homescreen notebook from list when this value set to true
    zeppelin.notebook.homescreen.hide: false
    # -- Output message from interpreter exceeding the limit will be truncated
    zeppelin.interpreter.output.limit: 102400
  server:
    # -- Zeppelin Java process memory options
    jvmMemOptions: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=1 -Xms512m -Xmx512m -XX:MaxMetaspaceSize=512m

  interpreter:
    enabled: true
    thriftPort: 10000

logging:
  level: INFO
  syslog: {}
    #level: INFO
    #host:
    #port:
    #facility:
    #pattern:

notebookStorage:
  # -- Kind of storage for notebook
  type: git
  # -- if type is `git` , use this secret to get the repository URL (usually the URL contains an access token)
  repositorySecret: git-data-lake

##
# Several ways to run spark notebook code:
# - driver (aka Zep' interpreter Spark application) can run 
#   - within zeppelin pod (mode `local`)
#   - as a separate Deployment (mode `external`, see zeppelin-int Helm chart)
# - worker(s) can run:
#   - zeppelin pod (`embedded` mode)
#   - zeppelin interpreter pod (`local` mode)
#   - Spark cluster
#   - k8s
##
spark:
  driver:
    # -- driver mode, `local` will run Zeppelin Spark interpreter in the same container, `external` will connect to an interpreter running in another pod
    mode: local # external
    # -- if mode is `local`, Spark UI port, usually 4040
    uiPort: 4040
    # -- if mode is `local`, create an ingress to access to Spark UI
    ingress:
      enabled: false
  executor:
    # -- `local` will run executor in the same driver java process / cluster / k8s
    mode: local
    # -- if mode is not local, full master URL
    masterURL: ""
    # -- if mode is not local, deploy mode (client / cluster)
    deployMode: client
    # -- if mode is not local, how many executors to deploy
    count: 2
    # -- how many cores (* to use all CPUs)
    # -- in `local` mode, this is equivalent to master = local[cores]
    cores: "*"
  # -- Spark configuration
  config: {}
  #  spark.hadoop.hive.metastore.uris: thrift://XXXX:9083
  #  spark.executor.memory: 1G
  #  zeppelin.spark.concurrentSQL: "true"
  #  spark.app.name: zep
  #  zeppelin.spark.enableSupportedVersionCheck: false
  # -- if driver mode is `local`, set which Spark image (version) to use to copy Spark home
  homeArtifacts:
    image:
      registry: docker.io
      repository: bitnami/spark
      pullPolicy: IfNotPresent
      tag: 2.4.5
    copyDirectory: /opt/bitnami/spark

  # -- download dependencies via an init container to use proxy only for this
  dependencies: {}
  #  packages:
  #  - org.elasticsearch:elasticsearch-spark-20_2.10:7.14.0
  #  proxy:
  #    host: 10.100.X.X
  #    port: 8080
